# Nettoyage et Préparation des Données : Licenciements dans les Big Tech

## Introduction

Ce projet vise à nettoyer et préparer un dataset issu de Kaggle portant sur les licenciements au sein des grandes entreprises technologiques. L'objectif est d'obtenir un jeu de données de haute qualité, prêt à être utilisé pour des analyses approfondies, de la visualisation de données ou de la narration de données.

## Dataset

- Origine: Kaggle
- Thématique: Licenciements dans les Big Tech
- Objectif du nettoyage:
- Normalisation et standardisation des données
- Correction des erreurs et anomalies
- Suppression des doublons et des valeurs manquantes

## Outils Utilisés

- MySQL: Système de gestion de bases de données relationnelles utilisé pour stocker, manipuler et nettoyer les données.

## Processus de Nettoyage

- Importation des données:
- Chargement du dataset dans une base de données MySQL.
- Exploration des données:
- Analyse de la structure des données (types de colonnes, valeurs uniques).
- Identification des erreurs potentielles (valeurs aberrantes, incohérences).
- Nettoyage des données:
  - Correction des erreurs de format: Uniformisation des formats de dates, de nombres, etc.
  - Traitement des valeurs manquantes: Suppression ou imputation des valeurs manquantes en fonction de leur importance.
  - Suppression des doublons: Identification et suppression des enregistrements dupliqués.
  - Normalisation et standardisation: Mise en œuvre de techniques de normalisation et de standardisation pour rendre les données comparables.
  - Création de nouvelles variables:
  - Calcul de nouvelles variables pertinentes pour l'analyse (e.g., taux de licenciement par entreprise).
- Exportation des données nettoyées: Exportation des données nettoyées dans un format adapté à l'analyse ultérieure (CSV, Excel, etc.).

## Résultats Attendus

- Un dataset propre et structuré, prêt à être utilisé pour :
- Visualisation de données: Création de graphiques et de tableaux de bord pour explorer les données de manière interactive.
- Data storytelling: Construction de narratifs convaincants à partir des données pour communiquer les résultats de l'analyse.
- Modélisation prédictive: Entraînement de modèles de machine learning pour prédire de futurs événements.

## Limitations

- Qualité des données initiales: La qualité du dataset source peut influencer la qualité des résultats.
- Choix des méthodes de nettoyage: Les choix méthodologiques peuvent avoir un impact sur les résultats finaux.
  
## Perspectives

- Enrichissement du dataset: Intégration de données supplémentaires pour une analyse plus approfondie.
- Analyse avancée: Réalisation d'analyses statistiques et de modélisation prédictive.
